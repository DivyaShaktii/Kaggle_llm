{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking/notebook\n",
    "#public score : 0.591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install /kaggle/input/sci-llm-pip/bitsandbytes-0.40.0.post4-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import Template\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data_path = Path('/kaggle/input/kaggle-llm-science-exam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "llm_backbone = '/kaggle/input/h2ogpt-gm-oasst1-en-2048-open-llama-7b/h2ogpt-gm-oasst1-en-2048-open-llama-7b/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_backbone,use_fast=False,\n",
    "    trust_remote_code=True,padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_backbone,torch_dtype=torch.float16,\n",
    "    #load_in_4bit=True,device_map=\"cuda:0\",trust_remote_code=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    test = pd.read_csv(data_path / 'test.csv', index_col='id')\n",
    "    test[\"answer\"] = \"A\"\n",
    "else:\n",
    "    test = pd.read_csv(data_path / 'train.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Perplexity(nn.Module):\n",
    "    def __init__(self, reduce: bool = True):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        perplexity = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n",
    "        perplexity = torch.stack(perplexity, dim=0)\n",
    "        #perplexity = torch.exp(perplexity)\n",
    "        if self.reduce:\n",
    "            perplexity = torch.mean(perplexity)\n",
    "        return perplexity \n",
    "    \n",
    "perp = Perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Precision at k\"\"\"\n",
    "    assert k <= len(r)\n",
    "    assert k != 0\n",
    "    return sum(int(x) for x in r[:k]) / k\n",
    "\n",
    "def MAP_at_3(predictions, true_items):\n",
    "    \"\"\"Score is mean average precision at 3\"\"\"\n",
    "    U = len(predictions)\n",
    "    map_at_3 = 0.0\n",
    "    for u in range(U):\n",
    "        user_preds = predictions[u]\n",
    "        user_true = true_items[u]\n",
    "        user_results = [1 if item == user_true else 0 for item in user_preds]\n",
    "        for k in range(min(len(user_preds), 3)):\n",
    "            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n",
    "    return map_at_3 / U\n",
    "\n",
    "maps = []\n",
    "preds = []\n",
    "for idx, row in tqdm(test.iterrows(), total=len(test)):\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cols = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "        perps = []\n",
    "        samples = []\n",
    "        for col in cols:\n",
    "            samples.append(\"<|prompt|>\"+row[\"prompt\"]+\"</s><|answer|>\"+row[col])\n",
    "        inputs = tokenizer(samples, return_tensors=\"pt\", add_special_tokens=False, padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "        output = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        output = output.logits\n",
    "        labels = inputs[\"input_ids\"]\n",
    "        labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n",
    "        for j in range(len(cols)):\n",
    "            p = perp(output[j].unsqueeze(0), labels[j].unsqueeze(0))\n",
    "            perps.append(p.detach().cpu())\n",
    "            \n",
    "        del inputs\n",
    "        del labels\n",
    "        del output\n",
    "        del p\n",
    "\n",
    "    perps = np.array(perps)\n",
    "        \n",
    "    predictions = [np.array(cols)[np.argsort(perps)]]\n",
    "    preds.append(predictions)\n",
    "    tp = [row.answer]\n",
    "    map = MAP_at_3(predictions, tp)\n",
    "    maps.append(map)\n",
    "    print(np.mean(maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
