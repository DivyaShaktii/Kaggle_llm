{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# six_K = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/6000_train_examples.csv') \n",
    "# data_500 = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/extra_train_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "#train_df = pd.concat([six_K , data_500 , train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# The path of the model checkpoint we want to use\n",
    "model_dir = '/home/ds/Desktop/Kaggle_llm/deberta-v3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = 'ABCDE'\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "def preprocess(example):\n",
    "    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n",
    "    # so we'll copy our question 5 times before tokenizing\n",
    "    first_sentence = [example['prompt']] * 5\n",
    "    second_sentence = []\n",
    "    for option in options:\n",
    "        second_sentence.append(example[option])\n",
    "    # Our tokenizer will turn our text into token IDs BERT can understand\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_train_ds = train_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n",
    "# will dynamically pad our questions at batch-time so we don't have to make every question the length\n",
    "# of our longest question.\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(flattened_features,padding=self.padding,\n",
    "            max_length=self.max_length,pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',)\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config'> for this kind of AutoModel: AutoModelForMultipleChoice.\nModel type should be one of YosoConfig, NystromformerConfig, QDQBertConfig, FNetConfig, RemBertConfig, CanineConfig, RoFormerConfig, BigBirdConfig, ConvBertConfig, IBertConfig, MobileBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaXLConfig, XLMRobertaConfig, MegatronBertConfig, MPNetConfig, LongformerConfig, RobertaConfig, FlaubertConfig, SqueezeBertConfig, BertConfig, XLNetConfig, XLMConfig, ElectraConfig, FunnelConfig, Data2VecTextConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0b6d62bebae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# make prediction on the private dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForMultipleChoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMultipleChoice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2Config'> for this kind of AutoModel: AutoModelForMultipleChoice.\nModel type should be one of YosoConfig, NystromformerConfig, QDQBertConfig, FNetConfig, RemBertConfig, CanineConfig, RoFormerConfig, BigBirdConfig, ConvBertConfig, IBertConfig, MobileBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaXLConfig, XLMRobertaConfig, MegatronBertConfig, MPNetConfig, LongformerConfig, RobertaConfig, FlaubertConfig, SqueezeBertConfig, BertConfig, XLNetConfig, XLMConfig, ElectraConfig, FunnelConfig, Data2VecTextConfig."
     ]
    }
   ],
   "source": [
    "# Now we'll instatiate the model that we'll finetune on our public dataset, then use to\n",
    "# make prediction on the private dataset.\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The arguments here are selected to run quickly; feel free to play with them.\n",
    "model_dir = 'finetuned_bert'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally it's a bad idea to validate on your training set, but because our training set\n",
    "# for this problem is so small we're going to train on all our data.\n",
    "trainer = Trainer(model=model,args=training_args,train_dataset=tokenized_train_ds,\n",
    "                  eval_dataset=tokenized_train_ds,tokenizer=tokenizer,\n",
    "                data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training should take about a minute\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can actually make predictions on our questions\n",
    "predictions = trainer.predict(tokenized_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function gets the indices of the highest scoring answers for each row\n",
    "# and converts them back to our answer format (A, B, C, D, E)\n",
    "import numpy as np\n",
    "def predictions_to_map_output(predictions):\n",
    "    sorted_answer_indices = np.argsort(-predictions)\n",
    "    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n",
    "    top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n",
    "    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's double check our output looks correct:\n",
    "predictions_to_map_output(predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load up our test set to use our model on!\n",
    "# The public test.csv isn't the real dataset (it's actually just a copy of train.csv without the answer column)\n",
    "# but it has the same format as the real test set, so using it is a good way to ensure our code will work when we submit.\n",
    "test_df = pd.read_csv('/kaggle/input/kaggle-llm-science-exam/test.csv')\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are more verbose/elegant ways of doing this, but if we give our test set a random `answer` column\n",
    "# we can make predictions directly with our trainer.\n",
    "test_df['answer'] = 'A'\n",
    "\n",
    "# Other than that we'll preprocess it in the same way we preprocessed test.csv\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "tokenized_test_ds = test_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we'll generate our \"real\" predictions on the test set\n",
    "test_predictions = trainer.predict(tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create our submission using the id column from test.csv\n",
    "submission_df = test_df[['id']]\n",
    "submission_df['prediction'] = predictions_to_map_output(test_predictions.predictions)\n",
    "\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
